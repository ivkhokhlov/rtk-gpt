Видеокарты нужны для запуска нейронок локально, без обращения отправки кода во внешние источники.
Самые популярные приложения на данный момент
# text-generation-webui
https://github.com/oobabooga/text-generation-webui
Локальный интерфейс взаимодействия нападобие ChatGPT с расширенным функционалом.
![image](https://github.com/ivkhokhlov/rtk-gpt/assets/58159018/14c0b5eb-286d-4670-a7ec-a9f6b5c1c7b4)

## Основные фичи text-generation-webui:
**Gradio веб-интерфейс для больших языковых моделей**: Целью является стать аналогом AUTOMATIC1111/stable-diffusion-webui для генерации текста.
**3 режима интерфейса**: По умолчанию (два столбца), блокнот и чат.
**Множественные модели**: Поддерживает такие модели, как transformers, ExLlamaV2, CTransformers.
**Выпадающее меню**: Для быстрого переключения между различными моделями.
**LoRA**: Загрузка и выгрузка LoRAs на лету, обучение нового LoRA с использованием QLoRA.
**Точные шаблоны инструкций для режима чата**: Включая Llama-2-chat, Alpaca, Vicuna, WizardLM, StableLM и многие другие.
**Инференция на 4-бит, 8-бит и CPU**: Через библиотеку transformers.
**Использование моделей llama.cpp с samplers transformers**: С использованием загрузчика llamacpp_HF.
**Мультимодальные пайплайны**: Включая LLaVA и MiniGPT-4.
b: Фреймворк для добавления дополнительных функций.
**Пользовательские персонажи для чата**: Позволяет настроить внешний вид и поведение персонажей в чате. Ответ в зависимости от загатовленного характера, истории и описанной форме.
**Очень эффективное текстовое потоковое вещание**: Обеспечивает быстрое и плавное взаимодействие с пользователем.
**Вывод в формате Markdown с рендерингом LaTeX**: Например, для использования с GALACTICA.
**API**: Включая конечные точки для потоковой передачи через веб-сокеты.

# gpt-engineer
https://github.com/AntonOsika/gpt-engineer
https://gpt-engineer.readthedocs.io/
https://user-images.githubusercontent.com/4467025/243695075-6e362e45-4a94-4b0d-973d-393a31d92d9b.mov
Интерфейс для работы с моделями GPT-3 и GPT-4, позволяя генерировать, исравлять и анализировать код на основе введенных пользователем запросов. Локальная версия модели для Python на данный момент превосходит в некоторых бенчмарках GPT-4.
Приложение позволяет локально анализировать код на соответствие требованиям, предлагать и вносить улучшения, писать тесты и вспомогательные 
**Написание кода:**
Позволяет генерировать и дорабатывать приложение с нуля, основываясь на базовых запросах. 
**Исправление кода**:
Дополнительные функции для работы с существующей кодовой базой
- `setup_sys_prompt(dbs)`: Устанавливает системный запрос, объединяя preprompt генерации и философский preprompt.
- `simple_gen(ai, dbs)`: Запускает AI на основном запросе и сохраняет результаты.
- `clarify(ai)`: Спрашивает у пользователя, хочет ли он что-то уточнить, и сохраняет результаты в рабочей области.
- `gen_spec(ai)`: Генерирует спецификацию из основного запроса + уточнений и сохраняет результаты в рабочей области.
- `respec(ai)`: Просит AI рассмотреть спецификацию новой функции и дать обратную связь по ней.
- `gen_unit_tests(ai)`: Генерирует модульные тесты на основе спецификации.
- `gen_clarified_code(ai)`: Генерирует код на основе основного запроса и уточнений.
- `gen_code(ai)`: Генерирует код на основе спецификации и модульных тестов.
- `gen_entrypoint(ai)`: Генерирует точку входа для сгенерированного кода.
- `use_feedback(ai)`: Использует обратную связь от пользователя для улучшения сгенерированного кода.
- `fix_code(ai)`: Исправляет любые ошибки в сгенерированном коде.
### Требования к железу
Локальные модели загружаются в VRAM и требуют большой объем видеопамяти для работы. 
В случае работы только на CPU (с использованием RAM системы) использование значительно затрудняется и время работы возрастает в несколько десятков раз.
Модели разделяются по количеству параметров, например 7B - 7 миллиардов параметров, 13B - 13 миллиардов и так далее.
Требования к GPU зависят от количества параметров и размера используемой модели GPTQ. Если использовать ExLlama, которая на данный момент является наиболее производительной и эффективной библиотекой GPTQ, то:
- 7B требует карты объемом 6 ГБ
- 13B требует карту объемом 10 ГБ
- 30B/33B - карта объемом 24 ГБ или 2 x 12 ГБ.
- 65B/70B - карта 48 ГБ или 2 x 24 ГБ.
**Самые производительные модели на данный момент:** 
общего назначения: Falcon-180B
Code LLama Python 65B

# stable-diffusion-webui
Генерация картинок по запросу с развитым API и веб-интерфейсом.

# Примерный конфига
